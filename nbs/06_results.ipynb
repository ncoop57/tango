{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "> This module contains all the code for running our experiments for Tango. To reproduce our results, please run each of the cells in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from two_to_tango.prep import *\n",
    "from two_to_tango.features import *\n",
    "from two_to_tango.eval import *\n",
    "from two_to_tango.model import *\n",
    "from two_to_tango.approach import *\n",
    "from two_to_tango.combo import *\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "path = Path(\"/tf/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup\n",
    "### Description:\n",
    "* Number of Participants: 14 (10 Ph.D. Students/Developers and 4 authors)\n",
    "* Number of Applications: 6\n",
    "* Number of Bug Reports per Application: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_user = 'user'\n",
    "vid_user_ds = VideoDataset.from_path(path/\"artifacts/videos\", fr = fps).label_from_paths()\n",
    "vid_user_ds.get_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Visual-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "### Configurations:\n",
    "* Number of Visual Words: 1,000, 5,000, 10,000\n",
    "* Codebook Number of Image Samples: MAX ~50,000\n",
    "* Number of frames kept: 1, 5\n",
    "* Model + Bag of Visual Words\n",
    "* Model + Fuzzy LCS\n",
    "* Model + LCS\n",
    "* Model + Bag of Visual Words + Fuzzy LCS\n",
    "* Model + Bag of Visual Words + Weight LCS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT - M00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_00 = 'SIFT'\n",
    "M00 = SIFTExtractor(cv2.xfeatures2d.SIFT_create(nfeatures = 10)) # limit SIFT features to top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimCLR - M01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_01 = 'SimCLR'\n",
    "simclr = SimCLRModel.load_from_checkpoint(checkpoint_path = str(path/'artifacts/models/SimCLR/checkpointepoch=98.ckpt')).eval()\n",
    "M01 = SimCLRExtractor(simclr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change these parameters if you want to only run a subset of our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vwords = [1_000, 5_000, 10_000]\n",
    "n_imgs = 15_000\n",
    "n_frames_to_keep = [1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rankings(\n",
    "    path, vid_ds, ds_name, model_name, model, sim_func, vwords, n_imgs,\n",
    "     n_frames_to_keep, fps\n",
    "):\n",
    "\n",
    "    for vw in tqdm(vwords):\n",
    "        for ftk in tqdm(n_frames_to_keep):\n",
    "            evaluation_metrics = {}\n",
    "            fname = path/f'artifacts/models/{model_name}/cookbook_{model_name}_{vw}vw.model'\n",
    "            codebook = pickle.load(open(fname, 'rb'))\n",
    "            start = time.time()\n",
    "            vid_ds_features = gen_extracted_features(vid_ds, model, fps, ftk)\n",
    "            end = time.time()\n",
    "            feature_gen_time = end - start\n",
    "            df, bovw_vid_ds_sims = gen_bovw_similarity(vid_ds, vid_ds_features, model, codebook, vw, ftk)\n",
    "            lcs_vid_ds_sims = gen_lcs_similarity(vid_ds, vid_ds_features, sim_func, model, codebook, df, vw, ftk)\n",
    "            \n",
    "            rankings = approach(\n",
    "                vid_ds, vid_ds_features, bovw_vid_ds_sims, lcs_vid_ds_sims, model, sim_func,\n",
    "                codebook, df, vw, fps = fps, ftk = ftk,\n",
    "            )\n",
    "            \n",
    "            evaluation_metrics['bovw'] = evaluate(\n",
    "                rankings['bovw']\n",
    "            )\n",
    "            evaluation_metrics['lcs'] = evaluate(\n",
    "                rankings['lcs']\n",
    "            )\n",
    "            evaluation_metrics['weighted_lcs'] = evaluate(\n",
    "                rankings['weighted_lcs']\n",
    "            )\n",
    "            \n",
    "            evaluation_metrics['bovw_lcs'] = evaluate(\n",
    "                rankings['bovw_lcs']\n",
    "            )\n",
    "            evaluation_metrics['bovw_weighted_lcs'] = evaluate(\n",
    "                rankings['bovw_weighted_lcs']\n",
    "            )\n",
    "            \n",
    "            id_name = f'{ds_name}_{n_imgs}n_{vw}vw_{ftk}ftk'\n",
    "            with open(path/f'outputs/results/{model_name}/rankings_{id_name}.pkl', 'wb') as f:\n",
    "                pickle.dump(rankings, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(path/f'outputs/results/{model_name}/evaluation_metrics_{id_name}.pkl', 'wb') as f:\n",
    "                pickle.dump(evaluation_metrics, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SIFT (M00) model takes a significant amount of time to run (>24 hours) on our machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_rankings(\n",
    "    path, vid_user_ds, ds_user, model_00, M00, sift_frame_sim, vwords, n_imgs,\n",
    "    n_frames_to_keep, fps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimCLR (M01) model is a lot faster than SIFT (~6 hours) on our machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_rankings(\n",
    "    path, vid_user_ds, ds_user, model_01, M01, simclr_frame_sim, vwords, n_imgs,\n",
    "    n_frames_to_keep, fps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_results(evals, app, item):\n",
    "    for bug in evals[app]:\n",
    "        if bug == 'elapsed_time' or bug == 'Bug Hit@1' \\\n",
    "        or bug == 'Bug Hit@5' or bug == 'Bug Hit@10' \\\n",
    "        or bug == 'App std rank' or bug == 'App mean rank' \\\n",
    "        or bug == 'App median rank' or bug == 'App mRR' \\\n",
    "        or bug == 'App mAP' or bug == 'App Hit@1' \\\n",
    "        or bug == 'App Hit@5' or bug == 'App Hit@10': continue\n",
    "        for vid in evals[app][bug]:\n",
    "            try:\n",
    "                print(evals[app][bug][vid][item])\n",
    "            except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name = f'{ds_user}_15000n_1000vw_5ftk'\n",
    "fname = path/f'outputs/results/{model_01}/evaluation_metrics_{id_name}.pkl'\n",
    "evals = pickle.load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_eval_results(evals['weighted_lcs'], 'APOD', 'rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Textual-Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate the settings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_file_path = path/'artifacts/user_assignment.csv'\n",
    "settings_path = path/'outputs/evaluation_settings'\n",
    "video_data = read_video_data(csv_file_path)\n",
    "generate_setting2(video_data, settings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert results from model to format of the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_path = path/'outputs/results'\n",
    "out_path = path/'outputs'\n",
    "models = ['SimCLR']\n",
    "convert_results_format(sim_path, settings_path, out_path, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run OCR text extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = path/'artifacts/videos'\n",
    "txt_out_path = path/'outputs/extracted_text'\n",
    "get_all_texts(vid_path, out_path, fps = 1)\n",
    "get_all_texts(vid_path, out_path, fps = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run text preprocessing, build document index, and run lucene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {path}/artifacts/models/tango_txt/\n",
    "! sh build_run.sh {txt_out_path} {settings_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs results to /tf/data/artifacts/tango_txt/tango_txt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Textual Information\n",
    "\n",
    "1. Compute the combination of visual and textual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_out_path = path/'outputs/combined'\n",
    "dl_ranking_path = path/'outputs/user_rankings_weighted_all/all_rankings.csv'\n",
    "ir_rankings_path = path/'artifacts/models/tango_txt/tango_txt_rankings/all_rankings.json'\n",
    "\n",
    "best_dl_models = [\n",
    "    \"SimCLR-1000vw-5ftk-bovw\", \"SimCLR-5000vw-5ftk-bovw_lcs\",\n",
    "    \"SimCLR-5000vw-5ftk-bovw_weighted_lcs\", \"SimCLR-1000vw-5ftk-bovw_weighted_lcs\"\n",
    "]\n",
    "best_ir_models = [\n",
    "    \"ocr+ir--1ftk-all_text\", \"ocr+ir--5ftk-all_text\",\n",
    "    \"ocr+ir--5ftk-unique_frames\", \"ocr+ir--5ftk-unique_words\"\n",
    "]\n",
    "\n",
    "tango_combined(combo_out_path, dl_ranking_path, ir_rankings_path, settings_path, best_dl_models, best_ir_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
