{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "> This module contains all the necessary functions for evaluating different video duplication detection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import cv2\n",
    "import ffmpeg\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from two_to_tango.prep import *\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path = Path(\"<path>\")\n",
    "video_paths = sorted(path.glob(\"**/*.mp4\")); video_paths[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_tf_idf(tfs, dfs):\n",
    "    tf_idf = np.array([])\n",
    "    for tf, df in zip(tfs, dfs):\n",
    "        tf = tf / np.sum(tfs)\n",
    "        idf = np.log(len(tfs) / (df + 1))\n",
    "        tf_idf = np.append(tf_idf, tf * idf)\n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hit_rate_at_k(rs, k):\n",
    "    hits = 0\n",
    "    for r in rs:\n",
    "        if np.sum(r[:k]) > 0: hits += 1\n",
    "    \n",
    "    return hits / len(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following methods from: https://gist.github.com/bwhite/3726239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def mean_reciprocal_rank(rs):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.5\n",
    "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.75\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "\n",
    "def r_precision(r):\n",
    "    \"\"\"Score is precision after all relevant documents have been retrieved\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> r_precision(r)\n",
    "    0.33333333333333331\n",
    "    >>> r = [0, 1, 0]\n",
    "    >>> r_precision(r)\n",
    "    0.5\n",
    "    >>> r = [1, 0, 0]\n",
    "    >>> r_precision(r)\n",
    "    1.0\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        R Precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    z = r.nonzero()[0]\n",
    "    if not z.size:\n",
    "        return 0.\n",
    "    return np.mean(r[:z[-1] + 1])\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> precision_at_k(r, 1)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 2)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 3)\n",
    "    0.33333333333333331\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.78333333333333333\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.39166666666666666\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "def recall_at_k(r, k, l):\n",
    "    \"\"\"Score is recall @ k\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> recall_at_k(r, 1, 2)\n",
    "    0.0\n",
    "    >>> recall_at_k(r, 2, 2)\n",
    "    0.0\n",
    "    >>> recall_at_k(r, 3, 2)\n",
    "    0.5\n",
    "    >>> recall_at_k(r, 4, 2)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: the length or size of the relevant items\n",
    "\n",
    "    Returns:\n",
    "        Recall @ k\n",
    "\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    assert l >= 1\n",
    "    r2 = np.asarray(r)[:k] != 0\n",
    "    if r2.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.sum(r2)/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = [[1, 0, 0], [0, 1, 0], [0, 0, 0]]\n",
    "mean_reciprocal_rank(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "average_precision(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rank_stats(rs):\n",
    "    ranks = []\n",
    "    for r in rs:\n",
    "        ranks.append(r.nonzero()[0][0] + 1)\n",
    "    ranks = np.asarray(ranks)\n",
    "    recipical_ranks = 1 / ranks\n",
    "    return np.std(ranks), np.mean(ranks), np.median(ranks), np.mean(recipical_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate(rankings, top_k = [1, 5, 10]):\n",
    "    output = {}\n",
    "    for app in rankings:\n",
    "        output[app] = {}\n",
    "        app_rs = []\n",
    "        for bug in rankings[app]:\n",
    "            if bug == 'elapsed_time': continue\n",
    "            output[app][bug] = {}\n",
    "            bug_rs = []\n",
    "            for report in rankings[app][bug]:\n",
    "                output[app][bug][report] = {'ranks': []}\n",
    "                r = []\n",
    "                for labels, score in rankings[app][bug][report].items():\n",
    "                    output[app][bug][report]['ranks'].append((labels, score))\n",
    "                    if labels[0] == bug: r.append(1)\n",
    "                    else: r.append(0)\n",
    "                r = np.asarray(r)\n",
    "                output[app][bug][report]['rank'] = r.nonzero()[0][0] + 1\n",
    "                output[app][bug][report]['average_precision'] = average_precision(r)\n",
    "                bug_rs.append(r)\n",
    "\n",
    "            bug_rs_std, bug_rs_mean, bug_rs_med, bug_mRR = rank_stats(bug_rs)\n",
    "            bug_mAP = mean_average_precision(bug_rs)\n",
    "\n",
    "            output[app][bug]['Bug std rank'] = bug_rs_std\n",
    "            output[app][bug]['Bug mean rank'] = bug_rs_mean\n",
    "            output[app][bug]['Bug median rank'] = bug_rs_med\n",
    "            output[app][bug]['Bug mRR'] = bug_mRR\n",
    "            output[app][bug]['Bug mAP'] = bug_mAP\n",
    "            for k in top_k:\n",
    "                bug_hit_rate = hit_rate_at_k(bug_rs, k)\n",
    "                output[app][f'Bug Hit@{k}'] = bug_hit_rate\n",
    "            app_rs.extend(bug_rs)\n",
    "\n",
    "        app_rs_std, app_rs_mean, app_rs_med, app_mRR = rank_stats(app_rs)\n",
    "        app_mAP = mean_average_precision(app_rs)\n",
    "\n",
    "        output[app]['App std rank'] = app_rs_std\n",
    "        output[app]['App mean rank'] = app_rs_mean\n",
    "        output[app]['App median rank'] = app_rs_med\n",
    "        output[app]['App mRR'] = app_mRR\n",
    "        output[app]['App mAP'] = app_mAP\n",
    "        print(f'{app} Elapsed Time in Seconds', rankings[app]['elapsed_time'])\n",
    "        print(f'{app} σ Rank', app_rs_std)\n",
    "        print(f'{app} μ Rank', app_rs_mean)\n",
    "        print(f'{app} Median Rank', app_rs_med)\n",
    "        print(f'{app} mRR:', app_mRR)\n",
    "        print(f'{app} mAP:', app_mAP)\n",
    "        for k in top_k:\n",
    "            app_hit_rate = hit_rate_at_k(app_rs, k)\n",
    "            output[app][f'App Hit@{k}'] = app_hit_rate\n",
    "            print(f'{app} Hit@{k}:', app_hit_rate)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_eval_results(evals, app, item):\n",
    "    for bug in evals[app]:\n",
    "        if bug == 'elapsed_time': continue\n",
    "        for vid in evals[app][bug]:\n",
    "            try:\n",
    "                print(evals[app][bug][vid][item])\n",
    "            except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_ranking(ranking, ground_truth):\n",
    "    relevance = []\n",
    "    for doc in ranking:\n",
    "        if doc in ground_truth:\n",
    "            relevance.append(1)\n",
    "        else:\n",
    "            relevance.append(0)\n",
    "\n",
    "    r = np.asarray(relevance)\n",
    "    first_rank = int(r.nonzero()[0][0] + 1)\n",
    "    avg_precision = average_precision(r)\n",
    "    recip_rank = 1 / first_rank\n",
    "\n",
    "    ranks = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    limit = 10\n",
    "\n",
    "    for k in range(1, limit + 1):\n",
    "        ranks.append(1 if first_rank <= k else 0)\n",
    "        precisions.append(precision_at_k(r, k))\n",
    "        recalls.append(recall_at_k(r, k, len(ground_truth)))\n",
    "\n",
    "    results = {\n",
    "        'first_rank': first_rank,\n",
    "        'recip_rank': recip_rank,\n",
    "        'avg_precision': avg_precision\n",
    "    }\n",
    "\n",
    "    for i in range(limit):\n",
    "        k = i + 1\n",
    "        results[\"rr@\" + str(k)] = ranks[i]\n",
    "        results[\"p@\" + str(k)] = precisions[i]\n",
    "        results[\"r@\" + str(k)] = recalls[i]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_prep.ipynb.\n",
      "Converted 01_features.ipynb.\n",
      "Converted 02_eval.ipynb.\n",
      "Converted 03_model.ipynb.\n",
      "Converted 04_approach.ipynb.\n",
      "Converted 05_cli.ipynb.\n",
      "Converted 06_results.ipynb.\n",
      "Converted 07_utils.ipynb.\n",
      "Converted 08_combo.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
