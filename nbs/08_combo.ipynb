{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinations\n",
    "\n",
    "> This module contains all the code for running our experiments for Tango. To reproduce our results, please run each of the cells in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import ntpath\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from two_to_tango.eval import *\n",
    "from two_to_tango.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "path = Path('/tf/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def execute_retrieval_run(run, similarities):\n",
    "    ranking = {}\n",
    "    query = run[\"query\"]\n",
    "    corpus = run[\"dup_corpus\"] + run[\"non_dup_corpus\"]\n",
    "\n",
    "    query_tokens = query.split(\"-\")\n",
    "    query_sims = similarities[query_tokens[0]][query_tokens[1]][query_tokens[2]]\n",
    "\n",
    "    for doc in corpus:\n",
    "        doc_tokens = doc.split(\"-\")\n",
    "        ranking[doc] = query_sims[(doc_tokens[1], doc_tokens[2])]\n",
    "\n",
    "    ranking = OrderedDict(sorted(ranking.items(), key=lambda t: t[1], reverse=True))\n",
    "    return ranking\n",
    "\n",
    "\n",
    "def run_settings(settings, similarities, config, systems_allowed=[]):\n",
    "    all_results = {}\n",
    "    all_rankings = {}\n",
    "    for setting in settings:\n",
    "        all_results[setting] = []\n",
    "        all_rankings[setting] = []\n",
    "\n",
    "        runs = settings[setting]\n",
    "\n",
    "        print(\"Running setting\", setting)\n",
    "        for run in runs:\n",
    "            query = run[\"query\"]\n",
    "            query_tokens = query.split(\"-\")\n",
    "\n",
    "            if len(systems_allowed) != 0 and query_tokens[0] not in systems_allowed:\n",
    "                continue\n",
    "\n",
    "            ranking = execute_retrieval_run(run, similarities)\n",
    "            ranking_results = evaluate_ranking(ranking, run[\"gnd_trh\"])\n",
    "\n",
    "            ranking_results[\"setting\"] = setting\n",
    "            ranking_results[\"app\"] = query_tokens[0]\n",
    "            ranking_results[\"run_id\"] = run[\"run_id\"]\n",
    "            ranking_results.update(config)\n",
    "\n",
    "            ranking_info = {\"run_id\": run[\"run_id\"], \"query\": query, \"ranking\": ranking}\n",
    "            ranking_info[\"setting\"] = setting\n",
    "            ranking_info.update(config)\n",
    "\n",
    "            all_results[setting].append(ranking_results)\n",
    "            all_rankings[setting].append(ranking_info)\n",
    "\n",
    "    return all_results, all_rankings\n",
    "\n",
    "\n",
    "def write_results(output_path, results):\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    all_results = []\n",
    "    for setting in results:\n",
    "        pd.read_json(json.dumps(results[setting])).to_csv(os.path.join(output_path, setting + '.csv'),\n",
    "                                                          index=False, sep=\";\")\n",
    "        all_results.extend(results[setting])\n",
    "\n",
    "    pd.read_json(json.dumps(all_results)).to_csv(os.path.join(output_path, 'all_results.csv'),\n",
    "                                                 index=False, sep=\";\")\n",
    "\n",
    "def write_rankings(output_path, rankings):\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    all_rankings = []\n",
    "    for setting in rankings:\n",
    "        write_json_line_by_line(rankings[setting], os.path.join(output_path, setting + '.csv'))\n",
    "        all_rankings.extend(rankings[setting])\n",
    "    write_json_line_by_line(all_rankings, os.path.join(output_path, 'all_rankings.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def convert_results_format(sim_path, settings_path, out_path, models):\n",
    "    similarities_path = sim_path\n",
    "    output_results = out_path/\"user_results_weighted_all\"\n",
    "    output_rankings = out_path/\"user_rankings_weighted_all\"\n",
    "    techniques = [\"weighted_lcs\", \"bovw\", \"lcs\", \"bovw_lcs\", \"bovw_weighted_lcs\"]\n",
    "    systems_allowed = []\n",
    "\n",
    "    settings_path = settings_path\n",
    "    settings = load_settings(settings_path)\n",
    "\n",
    "    all_results = {}\n",
    "    all_rankings = {}\n",
    "\n",
    "    for setting in settings:\n",
    "        all_results[setting] = []\n",
    "        all_rankings[setting] = []\n",
    "\n",
    "    for model in models:\n",
    "        sim_files = find_file(\"rankings_user_*.pkl\", os.path.join(similarities_path, model))\n",
    "        for sim_file in sim_files:\n",
    "            file_name = ntpath.basename(sim_file).split(\".\")[0]\n",
    "            file_tokens = file_name.split(\"_\")\n",
    "\n",
    "            vwords = file_tokens[3]\n",
    "            frames_per_sec = file_tokens[4]\n",
    "\n",
    "            model_similarities = pickle.load(open(sim_file, 'rb'))\n",
    "\n",
    "            for technique in techniques:\n",
    "                print(model_similarities.keys())\n",
    "                similarities = model_similarities[technique]\n",
    "                configuration = {\n",
    "                    \"model\": model,\n",
    "                    \"vwords\": vwords,\n",
    "                    \"fps\": frames_per_sec,\n",
    "                    \"technique\": technique\n",
    "                }\n",
    "\n",
    "                print(\"Running config: \", configuration)\n",
    "\n",
    "                results, rankings = run_settings(settings, similarities, configuration, systems_allowed)\n",
    "\n",
    "                for setting in settings:\n",
    "                    all_results[setting].extend(results[setting])\n",
    "                    all_rankings[setting].extend(rankings[setting])\n",
    "\n",
    "    print(\"Writing results and rankings\")\n",
    "\n",
    "    write_results(output_results, all_results)\n",
    "    write_rankings(output_rankings, all_rankings)\n",
    "\n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_path = path/'outputs/results'\n",
    "settings_path = path/'outputs/evaluation_settings'\n",
    "out_path = path/'outputs'\n",
    "models = ['SimCLR']\n",
    "convert_results_format(sim_path, settings_path, out_path, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_info_to_ranking_results(ranking, ranking_results, run, dl_model, ir_model, weight_str, setting):\n",
    "    new_model = dl_model[0] + \"-\" + ir_model[0]\n",
    "    new_vwords = dl_model[1]\n",
    "    new_fps = dl_model[2] + \"-\" + ir_model[1] + \"ftk\"\n",
    "    new_technique = dl_model[3] + \"-\" + ir_model[2]\n",
    "    new_config = \"({},{})\".format(\"-\".join(dl_model), \"-\".join(ir_model))\n",
    "    new_config_weight = \"({},{},{})\".format(weight_str, \"-\".join(dl_model), \"-\".join(ir_model))\n",
    "    config = {\n",
    "        \"model\": new_model,\n",
    "        \"vwords\": new_vwords,\n",
    "        \"fps\": new_fps,\n",
    "        \"technique\": new_technique,\n",
    "        \"weight\": weight_str,\n",
    "        \"model_config\": new_config,\n",
    "        \"model_config_weight\": new_config_weight\n",
    "    }\n",
    "\n",
    "    query = run[\"query\"]\n",
    "    query_tokens = query.split(\"-\")\n",
    "\n",
    "    ranking_results[\"setting\"] = setting\n",
    "    ranking_results[\"app\"] = query_tokens[0]\n",
    "    ranking_results[\"run_id\"] = run[\"run_id\"]\n",
    "    ranking_results.update(config)\n",
    "\n",
    "    ranking_info = {\"run_id\": run[\"run_id\"], \"query\": query, \"ranking\": ranking}\n",
    "    ranking_info.update(config)\n",
    "\n",
    "    return ranking_info, ranking_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tango_combined(out_path, dl_rankings_path, ir_rankings_path, settings_path, dl_models, ir_models):\n",
    "    # all_data\n",
    "    results_out_path = out_path/\"tango_comb_results\"\n",
    "    rankings_out_path = out_path/\"tango_comb_rankings\"\n",
    "\n",
    "    # calibration\n",
    "    # settings_path = 'evaluation_settings_split/calibration'\n",
    "    # results_out_path = \"comb_results_calib\"\n",
    "    # rankings_out_path = \"comb_rankings_calib\"\n",
    "\n",
    "    # test\n",
    "    # settings_path = 'evaluation_settings_split/test'\n",
    "    # results_out_path = \"comb_results_test\"\n",
    "    # rankings_out_path = \"comb_rankings_test\"\n",
    "\n",
    "    Path(results_out_path).mkdir(parents=True, exist_ok=True)\n",
    "    Path(rankings_out_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # read data\n",
    "    settings = load_settings(settings_path)\n",
    "\n",
    "    dl_rankings = read_json_line_by_line(dl_rankings_path)\n",
    "    dl_rankings_by_config = group_dict(dl_rankings, lambda rec: (rec['model'], rec['vwords'], rec['fps'],\n",
    "                                                                    rec['technique'],))\n",
    "\n",
    "    ir_rankings = read_json(ir_rankings_path)\n",
    "    ir_rankings_by_config = group_dict(ir_rankings, lambda rec: (rec['model'], rec['fps'],\n",
    "                                                                    rec['technique'],))\n",
    "\n",
    "#     best_dl_models = [\n",
    "#         \"M00-10000vw-1ftk-bovw_weighted_lcs\", \"M00-10000vw-1ftk-weighted_lcs\",\n",
    "#         \"M00-10000vw-5ftk-bovw_weighted_lcs\", \"M00-1000vw-1ftk-weighted_lcs\", \"M00-1000vw-5ftk-bovw\",\n",
    "#         \"M00-1000vw-5ftk-bovw_weighted_lcs\", \"M00-5000vw-5ftk-weighted_lcs\", \"M01-1000vw-5ftk-bovw\",\n",
    "#         \"M01-5000vw-5ftk-bovw_lcs\", \"M01-5000vw-5ftk-bovw_weighted_lcs\",\n",
    "#         \"M01-1000vw-5ftk-bovw_weighted_lcs\"]\n",
    "#     best_ir_models = [\"ocr+ir--1ftk-all_text\", \"ocr+ir--5ftk-all_text\", \"ocr+ir--5ftk-unique_frames\",\n",
    "#                       \"ocr+ir--5ftk-unique_words\"]\n",
    "\n",
    "    ir_model_apps_for_comb = {\n",
    "        \"1ftk-all_text\": ['APOD', 'DROID', 'GNU', 'GROW'],\n",
    "        \"5ftk-all_text\": ['APOD', 'DROID', 'GNU', 'GROW'],\n",
    "        \"5ftk-unique_frames\": ['APOD', 'DROID', 'GROW'],\n",
    "        \"5ftk-unique_words\": ['APOD', 'GROW'],\n",
    "    }\n",
    "\n",
    "    settings_to_run = [\"setting2\"]\n",
    "\n",
    "    dl_models = list(filter(lambda rec: \"-\".join([rec[0], rec[1], rec[2], rec[3]]) in dl_models,\n",
    "                            dl_rankings_by_config.keys()))\n",
    "    ir_models = list(filter(lambda rec: \"-\".join([rec[0], \"\", rec[1] + \"ftk\", rec[2]]) in ir_models,\n",
    "                            ir_rankings_by_config.keys()))\n",
    "\n",
    "    # run combinations\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_new_rankings = []\n",
    "    all_new_results = []\n",
    "    for dl_model in dl_models:\n",
    "        dl_mod_rankings = group_dict(dl_rankings_by_config[dl_model], lambda rec: rec[\"setting\"])\n",
    "        for ir_model in ir_models:\n",
    "            ir_mod_rankings = group_dict(ir_rankings_by_config[ir_model], lambda rec: rec[\"setting\"])\n",
    "\n",
    "            print(dl_model, ir_model)\n",
    "\n",
    "            app_for_comb = ir_model_apps_for_comb[\"-\".join([ir_model[1] + \"ftk\", ir_model[2]])]\n",
    "\n",
    "            for setting in settings_to_run:\n",
    "                dl_runs = group_dict(dl_mod_rankings[setting], lambda rec: rec[\"run_id\"])\n",
    "                ir_runs = group_dict(ir_mod_rankings[setting], lambda rec: rec[\"runId\"])\n",
    "\n",
    "                setting_runs = settings[setting]\n",
    "\n",
    "                for run in setting_runs:\n",
    "\n",
    "                    run_id = run[\"run_id\"]\n",
    "\n",
    "                    ir_run_ranking = ir_runs[str(run_id)][0][\"ranking\"]\n",
    "                    ir_run_ranking = dict(\n",
    "                        zip((rec[\"docName\"] for rec in ir_run_ranking), (rec for rec in ir_run_ranking)))\n",
    "                    dl_run_ranking = dl_runs[run_id][0][\"ranking\"]\n",
    "\n",
    "                    # rankings based on all weights\n",
    "                    for weight in np.arange(0, 1.1, 0.1):\n",
    "                        new_ranking = {}\n",
    "                        for doc in dl_run_ranking:\n",
    "                            ir_score = 0 if doc not in ir_run_ranking else ir_run_ranking[doc][\"score\"]\n",
    "                            dl_score = dl_run_ranking[doc]\n",
    "                            new_score = weight * ir_score + (1 - weight) * dl_score\n",
    "                            new_ranking[doc] = new_score\n",
    "\n",
    "                        ranking = OrderedDict(sorted(new_ranking.items(), key=lambda t: t[1], reverse=True))\n",
    "                        ranking_results = evaluate_ranking(ranking, run[\"gnd_trh\"])\n",
    "\n",
    "                        ranking_info, ranking_results = get_info_to_ranking_results(ranking, ranking_results,\n",
    "                                                                                    run, dl_model, ir_model,\n",
    "                                                                                    str(weight), setting)\n",
    "\n",
    "                        all_new_results.append(ranking_results)\n",
    "                        all_new_rankings.append(ranking_info)\n",
    "\n",
    "                # -------------------------------------------------------------\n",
    "\n",
    "                # rankings of approach based vocabulary agreement (e.g., 0.2-0: 0.2 weight for all apps except TIME,\n",
    "                # TOK, and 0 weight for TIME and TOK)\n",
    "                for run in setting_runs:\n",
    "\n",
    "                    run_id = run[\"run_id\"]\n",
    "\n",
    "                    ir_run_ranking = ir_runs[str(run_id)][0][\"ranking\"]\n",
    "                    ir_run_ranking = dict(\n",
    "                        zip((rec[\"docName\"] for rec in ir_run_ranking), (rec for rec in ir_run_ranking)))\n",
    "                    dl_run_ranking = dl_runs[run_id][0][\"ranking\"]\n",
    "\n",
    "                    query = run[\"query\"]\n",
    "                    query_tokens = query.split(\"-\")\n",
    "\n",
    "                    app = query_tokens[0]\n",
    "\n",
    "                    for base_weight in np.arange(0.1, 1.1, 0.1):\n",
    "\n",
    "                        best_weights_name = f'{base_weight:0.1f}' + \"-0\"\n",
    "\n",
    "                        weight = 0\n",
    "                        if app in app_for_comb:\n",
    "                            weight = base_weight\n",
    "\n",
    "                        new_ranking = {}\n",
    "                        for doc in dl_run_ranking:\n",
    "                            ir_score = 0 if doc not in ir_run_ranking else ir_run_ranking[doc][\"score\"]\n",
    "                            dl_score = dl_run_ranking[doc]\n",
    "                            new_score = weight * ir_score + (1 - weight) * dl_score\n",
    "                            new_ranking[doc] = new_score\n",
    "\n",
    "                        ranking = OrderedDict(sorted(new_ranking.items(), key=lambda t: t[1], reverse=True))\n",
    "                        ranking_results = evaluate_ranking(ranking, run[\"gnd_trh\"])\n",
    "\n",
    "                        ranking_info, ranking_results = get_info_to_ranking_results(ranking, ranking_results,\n",
    "                                                                                    run, dl_model, ir_model,\n",
    "                                                                                    best_weights_name, setting)\n",
    "\n",
    "                        all_new_results.append(ranking_results)\n",
    "                        all_new_rankings.append(ranking_info)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    print(\"Writing data\")\n",
    "\n",
    "    pd.read_json(json.dumps(all_new_results)).to_csv(os.path.join(results_out_path, 'all_results.csv'),\n",
    "                                                     index=False, sep=\";\")\n",
    "    write_json_line_by_line(all_new_rankings, os.path.join(rankings_out_path, 'all_rankings.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_ranking_path = path/'outputs/user_rankings_weighted_all/all_rankings.csv'\n",
    "ir_rankings_path = path/'models/tango_txt/tango_txt_rankings/all_rankings.json'\n",
    "settings_path = path/'outputs/evaluation_settings'\n",
    "\n",
    "tango_combined(dl_ranking_path, ir_rankings_path, settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
