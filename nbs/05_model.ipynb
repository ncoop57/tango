{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "\n",
    "> This module contains all the code for training and extracting features from CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "from nbdev.showdoc import *\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class SimCLRDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"Initialize a wrapper of a generic image classification dataset for SimCLR training.\n",
    "\n",
    "        Args:\n",
    "            dataset (torch.utils.data.Dataset): an image PyTorch dataset - when iterating over it\n",
    "                it should return something of the form (image) or (image, label).\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        dataset_item = self.dataset[index]\n",
    "        if type(dataset_item) is tuple:\n",
    "            image = dataset_item[0]\n",
    "        else:\n",
    "            image = dataset_item\n",
    "        return image, image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    @staticmethod\n",
    "    def mixup(x, alpha=0.4):\n",
    "        batch_size = x.size()[0] // 2\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha, batch_size)\n",
    "            lam = np.concatenate(\n",
    "                [lam[:, None], 1 - lam[:, None]], 1\n",
    "            ).max(1)[:, None, None, None]\n",
    "            lam = torch.from_numpy(lam).float()\n",
    "            if torch.cuda.is_available():\n",
    "                lam = lam.cuda()\n",
    "        else:\n",
    "            lam = 1.\n",
    "        # This is SimCLR specific - we want to use the same mixing for the augmented pairs\n",
    "        lam = torch.cat([lam, lam])\n",
    "        index = torch.randperm(batch_size)\n",
    "        # This is SimCLR specific - we want to use the same permutation on the augmented pairs\n",
    "        index = torch.cat([index, batch_size + index])\n",
    "        if torch.cuda.is_available():\n",
    "            index = index.cuda()\n",
    "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "\n",
    "        return mixed_x, lam\n",
    "\n",
    "\n",
    "def imagenet_normalize_transform():\n",
    "    return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def get_train_transforms(size=224, color_jitter_prob=0.8, grayscale_prob=0.2):\n",
    "    color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=(size, size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([color_jitter], p=color_jitter_prob),\n",
    "        transforms.RandomGrayscale(p=grayscale_prob),\n",
    "        transforms.ToTensor(),\n",
    "        imagenet_normalize_transform()\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms(size=224):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size=(size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        imagenet_normalize_transform()\n",
    "    ])\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NTXEntCriterion(nn.Module):\n",
    "    \"\"\"Normalized, temperature-scaled cross-entropy criterion, as suggested in the SimCLR paper.\n",
    "\n",
    "    Parameters:\n",
    "        temperature (float, optional): temperature to scale the confidences. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    similarity = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXEntCriterion, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.batch_size = None\n",
    "        self.mask = None\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        \"\"\"Masks examples in a batch and it's augmented pair for computing the valid summands for\n",
    "            the criterion.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): batch size of the individual batch (not including it's augmented pair)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: a mask (tensor of 0s and 1s), where 1s indicates a pair of examples in a\n",
    "                batch that will contribute to the overall batch loss\n",
    "        \"\"\"\n",
    "        mask = torch.ones((batch_size * 2, batch_size * 2), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def compute_similarities(self, z_i, z_j, temperature):\n",
    "        \"\"\"Computes the similarities between two projections `z_i` and `z_j`, scaling based on\n",
    "            `temperature`.\n",
    "\n",
    "        Args:\n",
    "            z_i (torch.Tensor): projection of a batch\n",
    "            z_j (torch.Tensor): projection of the augmented pair for the batch\n",
    "            temperature (float): temperature to scale the similarity by\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: tensor of similarities for the positive and negative pairs\n",
    "        \"\"\"\n",
    "        batch_size = len(z_i)\n",
    "        mask = self.mask_correlated_samples(batch_size)\n",
    "\n",
    "        p1 = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = self.similarity(p1.unsqueeze(1), p1.unsqueeze(0)) / temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, batch_size)\n",
    "        sim_j_i = torch.diag(sim, -batch_size)\n",
    "\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(\n",
    "            batch_size * 2, 1\n",
    "        )\n",
    "        negative_samples = sim[mask].reshape(batch_size * 2, -1)\n",
    "\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"Computes the loss for a batch and its augmented pair.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): tensor of a batch and it's augmented pair, concatenated\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: loss for the given batch\n",
    "        \"\"\"\n",
    "        double_batch_size = len(z)\n",
    "        batch_size = double_batch_size // 2\n",
    "        z_i, z_j = z[:double_batch_size // 2], z[double_batch_size // 2:]\n",
    "        if self.batch_size is None or batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "            self.mask = None\n",
    "\n",
    "        if self.mask is None:\n",
    "            self.mask = self.mask_correlated_samples(self.batch_size)\n",
    "\n",
    "        logits = self.compute_similarities(z_i, z_j, self.temperature)\n",
    "        labels = torch.zeros(self.batch_size * 2).long()\n",
    "        logits, labels = logits.to(z.device), labels.to(z.device)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= 2 * self.batch_size\n",
    "        return loss\n",
    "\n",
    "class SimCLRModel(LightningModule):\n",
    "    \"\"\"SimCLR training network for a generic torchvision model (restricted to `allowed_models`). \"\"\"\n",
    "\n",
    "    allowed_models = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']\n",
    "    allowed_datasets = ['CIFAR10', 'CIFAR100', 'STL10', 'SVHN']\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name='resnet18', pretrained=True, projection_dim=64, temperature=0.5,\n",
    "        batch_size=128, image_size=224, save_hparams=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = list(getattr(torchvision.models, model_name)(pretrained=pretrained).children())\n",
    "        self.model = nn.Sequential(*layers[:-1])\n",
    "        self.projection_head = nn.Linear(layers[-1].in_features, projection_dim)\n",
    "        self.loss = NTXEntCriterion(temperature=temperature)\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        out = out.view(x.size(0), -1)\n",
    "        out = self.projection_head(out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        projections = self(batch)\n",
    "        loss = self.loss(projections)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        self.logger.scalar('loss', loss)\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss_mean = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        return {'train_loss': loss_mean}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([\n",
    "            {'params': self.model.parameters(), 'lr': 0.00001},\n",
    "            {'params': self.projection_head.parameters(), 'lr': 0.001}\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        train_transforms, val_transforms = (\n",
    "            get_train_transforms(size=self.image_size),\n",
    "            get_val_transforms(size=self.image_size)\n",
    "        )\n",
    "        train_dataset = torchvision.datasets.ImageFolder(\n",
    "            '/tf/data/combined', transform = train_transforms)\n",
    "        self.train_dataset = SimCLRDataset(train_dataset)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        return torch.cat([torch.stack([b[0] for b in batch]), torch.stack([b[1] for b in batch])])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, num_workers=64, shuffle=True,\n",
    "            collate_fn=self.collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Hook():\n",
    "    \"Create a hook on `m` with `hook_func`.\"\n",
    "    def __init__(self, m:nn.Module, hook_func:HookFunc, is_forward:bool=True, detach:bool=True):\n",
    "        self.hook_func,self.detach,self.stored = hook_func,detach,None\n",
    "        f = m.register_forward_hook if is_forward else m.register_backward_hook\n",
    "        self.hook = f(self.hook_fn)\n",
    "        self.removed = False\n",
    "\n",
    "    def hook_fn(self, module:nn.Module, input:Tensors, output:Tensors):\n",
    "        \"Applies `hook_func` to `module`, `input`, `output`.\"\n",
    "        if self.detach:\n",
    "            input  = (o.detach() for o in input ) if is_listy(input ) else input.detach()\n",
    "            output = (o.detach() for o in output) if is_listy(output) else output.detach()\n",
    "        self.stored = self.hook_func(module, input, output)\n",
    "\n",
    "    def remove(self):\n",
    "        \"Remove the hook from the model.\"\n",
    "        if not self.removed:\n",
    "            self.hook.remove()\n",
    "            self.removed=True\n",
    "\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__(self, *args): self.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_output(module, input_value, output):\n",
    "    return output.flatten(1)\n",
    "\n",
    "def get_input(module, input_value, output):\n",
    "    return list(input_value)[0]\n",
    "\n",
    "def get_named_module_from_model(model, name):\n",
    "    for n, m in model.named_modules():\n",
    "        if n == name:\n",
    "            return m\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trainPetsModel():\n",
    "    '''Fine tunes a pretrained ResNet50 model to detect pet breeds \n",
    "    and returns the resulting model in learn as well as the last layer\n",
    "    in linear_output_layer for subsequent feature extraction.'''\n",
    "    \n",
    "    bs = 16\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    path = untar_data(URLs.PETS)\n",
    "    path_img = path/'images'\n",
    "    fnames = get_image_files(path_img)\n",
    "    pat = pat = r'/([^/]+)_\\d+.jpg$'\n",
    "    data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs\n",
    "                                  ).normalize(imagenet_stats)\n",
    "    learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
    "    learn.fit_one_cycle(4)\n",
    "\n",
    "    linear_output_layer = get_named_module_from_model(learn.model, '1.4')\n",
    "    return learn, linear_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def getFeaturesResNet50(img):\n",
    "    '''Directly gets a feature vector from the last layer of ResNet50\n",
    "    without any fine tuning.'''\n",
    "    \n",
    "    model = models.resnet50(pretrained=True)\n",
    "    modules = list(model.children())[:-1]\n",
    "    resNet50 = nn.Sequential(*modules)\n",
    "    imgVar = Variable(img)\n",
    "    featuresVar = resNet50.eval()(imgVar)\n",
    "    features = featuresVar.data\n",
    "    \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trainAsModel():\n",
    "    '''Fine tunes a pretrained ResNet50 model to detect Google Play store  \n",
    "    categories of Android screenshots and returns the resulting model in learn \n",
    "    as well as the last layer in linear_output_layer for subsequent feature extraction.'''\n",
    "    \n",
    "    bs = 16\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    path = \"/tf/GP-Labeled-Dataset/\"\n",
    "    data = ImageDataBunch.from_folder(path, ds_tfms=get_transforms(), size=299, bs=bs).normalize(imagenet_stats)\n",
    "    learn = cnn_learner(data, models.resnet50, metrics=error_rate)\n",
    "    learn.load('/tf/models/Android_ResNet50_16Epochs')\n",
    "    \n",
    "    linear_output_layer = get_named_module_from_model(learn.model, '1.4')\n",
    "    return learn, linear_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Extractor():\n",
    "    \n",
    "    '''Low level implementation of Extractor class for non-layered models only.'''\n",
    "    \n",
    "    def __init__(self, learn, linear_output_layer, model, isFT):\n",
    "        self.models = {'resnet50'}\n",
    "        self.learn = learn\n",
    "        self.linear_output_layer = linear_output_layer\n",
    "        self.model = model\n",
    "        self.isFT = isFT\n",
    "        \n",
    "        if self.model not in self.models:\n",
    "            raise Exception(\"Invalid architecture: \" + self.model)\n",
    "    \n",
    "    def getFeatures(self, img):\n",
    "        if self.model == 'resnet50':\n",
    "            resized_img = cv2.resize(img, (224, 224))\n",
    "        final_img = np.expand_dims(np.transpose(resized_img, [2, 0, 1]), axis=0)\n",
    "        tensor = torch.from_numpy(final_img)\n",
    "        \n",
    "        img_repr = []\n",
    "        if self.isFT:\n",
    "            floatTensor = tensor.type(torch.cuda.FloatTensor)\n",
    "            with Hook(self.linear_output_layer, get_output, True, True) as hook:\n",
    "                result = self.learn.model.eval()(floatTensor)\n",
    "                img_repr = hook.stored.cpu().numpy()\n",
    "        else:\n",
    "            floatTensor = tensor.type(torch.FloatTensor)\n",
    "            img_repr = getFeaturesResNet50(floatTensor).numpy().reshape(1, -1)\n",
    "        \n",
    "        return img_repr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LayeredExtractor():\n",
    "    \n",
    "    '''Low level implementation of Extractor class for layered models only.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.features = []\n",
    "        \n",
    "    def hook_fn(self, module, input, output):\n",
    "        output_flat = output.detach().flatten()\n",
    "        self.features = np.hstack((self.features, output_flat))\n",
    "\n",
    "    def getFeatures(self, img):\n",
    "        self.features = []\n",
    "        \n",
    "        resized_img = cv2.resize(img, (224, 224))\n",
    "        final_img = np.expand_dims(np.transpose(resized_img, [2, 0, 1]), axis=0)\n",
    "        tensor = torch.from_numpy(final_img)\n",
    "        floatTensor = tensor.type(torch.FloatTensor)\n",
    "        \n",
    "        model = models.resnet50(pretrained=True)\n",
    "        for n, m in model.named_modules():\n",
    "            if 'pool' in n:\n",
    "                m.register_forward_hook(self.hook_fn)\n",
    "        model.eval()(floatTensor)\n",
    "        self.features = self.features.reshape(1, -1)\n",
    "        return self.features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def createExtractor(learn, linear_output_layer, model, isFT):\n",
    "    return Extractor(learn, linear_output_layer, model, isFT)\n",
    "def createLayeredExtractor():\n",
    "    return LayeredExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_prep.ipynb.\n",
      "Converted 01_features.ipynb.\n",
      "Converted 02_eval.ipynb.\n",
      "Converted 03_cnn.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      " \n",
      "Converted 04_experiments.ipynb.\n",
      "Converted 05_model.ipynb.\n",
      "Converted SimCLR.old.ipynb.\n",
      "Converted [Scratch 1] Tango SimCLR.ipynb.\n",
      "Converted [Scratch 2] Tango SimCLR.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted lesson1-pets.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
